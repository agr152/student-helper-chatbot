This project is a Retrieval-Augmented Generation (RAG) chatbot built using Flask, LangChain, and ChromaDB, integrated with a local LLM via Ollama.
It enables users to ask questions and get context-aware responses based only on the documents stored in the vector database.

🚀 Features
✅ User-friendly Flask web interface for chatting
✅ LangChain integration for chaining prompt logic and document combination
✅ ChromaDB for efficient document vector storage and retrieval
✅ Uses Ollama-compatible LLM for answering questions
✅ Ensures answers are generated only from the relevant documents (context-based answering)
✅ Modular code structure for easy customization and scaling

🛠️ Tech Stack
✅ Backend: Python, Flask
✅ LLM & Embeddings: LangChain + Ollama
✅ Vector Store: ChromaDB
✅ Prompt Engineering: ChatPromptTemplate from LangChain
✅ Frontend: HTML (via Jinja templates)


Some of the Sample Screenshot of the UI below,
![Screenshot (63)](https://github.com/user-attachments/assets/77ade287-39db-4db2-b628-142b97161a66)

This how the conversation with chatbot looks like,
![Screenshot (64)](https://github.com/user-attachments/assets/f94572ca-f57c-4737-9ecd-e53cacecc490)



