This project is a Retrieval-Augmented Generation (RAG) chatbot built using Flask, LangChain, and ChromaDB, integrated with a local LLM via Ollama.
It enables users to ask questions and get context-aware responses based only on the documents stored in the vector database.

🚀 Features
✅ User-friendly Flask web interface for chatting
✅ LangChain integration for chaining prompt logic and document combination
✅ ChromaDB for efficient document vector storage and retrieval
✅ Uses Ollama-compatible LLM for answering questions
✅ Ensures answers are generated only from the relevant documents (context-based answering)
✅ Modular code structure for easy customization and scaling

🛠️ Tech Stack
✅ Backend: Python, Flask
✅ LLM & Embeddings: LangChain + Ollama
✅ Vector Store: ChromaDB
✅ Prompt Engineering: ChatPromptTemplate from LangChain
✅ Frontend: HTML (via Jinja templates)


Some of the Sample Screenshot of the UI below,
![Screenshot (63)](https://github.com/user-attachments/assets/05f61b74-5a04-41ec-bd34-f673b98ae9bb)

This how the conversation with chatbot looks like,
![Screenshot (64)](https://github.com/user-attachments/assets/672b2bcc-8b99-43a7-8116-9c098ffb0eb6)


