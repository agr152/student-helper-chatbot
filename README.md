This project is a Retrieval-Augmented Generation (RAG) chatbot built using Flask, LangChain, and ChromaDB, integrated with a local LLM via Ollama.
It enables users to ask questions and get context-aware responses based only on the documents stored in the vector database.

ğŸš€ Features
âœ… User-friendly Flask web interface for chatting
âœ… LangChain integration for chaining prompt logic and document combination
âœ… ChromaDB for efficient document vector storage and retrieval
âœ… Uses Ollama-compatible LLM for answering questions
âœ… Ensures answers are generated only from the relevant documents (context-based answering)
âœ… Modular code structure for easy customization and scaling

ğŸ› ï¸ Tech Stack
âœ… Backend: Python, Flask
âœ… LLM & Embeddings: LangChain + Ollama
âœ… Vector Store: ChromaDB
âœ… Prompt Engineering: ChatPromptTemplate from LangChain
âœ… Frontend: HTML (via Jinja templates)


Some of the Sample Screenshot of the UI below,
![Screenshot (63)](https://github.com/user-attachments/assets/77ade287-39db-4db2-b628-142b97161a66)

This how the conversation with chatbot looks like,
![Screenshot (64)](https://github.com/user-attachments/assets/f94572ca-f57c-4737-9ecd-e53cacecc490)



