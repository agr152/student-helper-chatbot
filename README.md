This project is a Retrieval-Augmented Generation (RAG) chatbot built using Flask, LangChain, and ChromaDB, integrated with a local LLM via Ollama.
It enables users to ask questions and get context-aware responses based only on the documents stored in the vector database.

ğŸš€ Features
âœ… User-friendly Flask web interface for chatting
âœ… LangChain integration for chaining prompt logic and document combination
âœ… ChromaDB for efficient document vector storage and retrieval
âœ… Uses Ollama-compatible LLM for answering questions
âœ… Ensures answers are generated only from the relevant documents (context-based answering)
âœ… Modular code structure for easy customization and scaling

ğŸ› ï¸ Tech Stack
âœ… Backend: Python, Flask
âœ… LLM & Embeddings: LangChain + Ollama
âœ… Vector Store: ChromaDB
âœ… Prompt Engineering: ChatPromptTemplate from LangChain
âœ… Frontend: HTML (via Jinja templates)


Some of the Sample Screenshot of the UI below,
![Screenshot (63)](https://github.com/user-attachments/assets/05f61b74-5a04-41ec-bd34-f673b98ae9bb)

This how the conversation with chatbot looks like,
![Screenshot (64)](https://github.com/user-attachments/assets/672b2bcc-8b99-43a7-8116-9c098ffb0eb6)


